{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the environment where the wri project is located. If there is not one, install requirements in the wrilatinamerica.txt. https://github.com/OmdenaAI/wrilatinamerica/blob/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda activate wri_omdena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import segment_highlighter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import string\n",
    "\n",
    "import nltk # imports the natural language toolkit\n",
    "from nltk.corpus import stopwords\n",
    "import plotly\n",
    "from wordcloud import WordCloud\n",
    "import es_core_news_md\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from PyPDF2 import PdfFileReader\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we use the excel file with the selected phrases of each country, process them and get N-grams to define basic queries for the SBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Text</th>\n",
       "      <th>Incentive Instrument</th>\n",
       "      <th>Land Use Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Unique Policy #</th>\n",
       "      <th>Key words</th>\n",
       "      <th>relevant sentences</th>\n",
       "      <th>Key Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019 ACUERDO por el que se emiten los Lineamie...</td>\n",
       "      <td>Generar empleo y garantizara la población camp...</td>\n",
       "      <td>Direct payment (PES), Credit, Technical assist...</td>\n",
       "      <td>Forest, Agriculture (Crop)</td>\n",
       "      <td>Incentive</td>\n",
       "      <td>1 (Sembrando Vida)</td>\n",
       "      <td>insumo, crédito, capacitación, asistencia técnica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019 ACUERDO por el que se emiten los Lineamie...</td>\n",
       "      <td>\\nEl Programa incentivará a los sujetos agrari...</td>\n",
       "      <td>Direct payment (PES), Credit, Technical assist...</td>\n",
       "      <td>Forest, Agriculture (Crop)</td>\n",
       "      <td>Incentive</td>\n",
       "      <td>1 (Sembrando Vida)</td>\n",
       "      <td>incentivar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019 ACUERDO por el que se emiten los Lineamie...</td>\n",
       "      <td>Los sujetos agrarios beneficiados por el progr...</td>\n",
       "      <td>Supplies, Technical assistance</td>\n",
       "      <td>Forest, Agriculture (Crop)</td>\n",
       "      <td>Incentive</td>\n",
       "      <td>1 (Sembrando Vida)</td>\n",
       "      <td>apoyo económico, apoyos en especie, insumos, h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019 ACUERDO por el que se emiten los Lineamie...</td>\n",
       "      <td>El sujeto de derecho, recibirá un apoyo económ...</td>\n",
       "      <td>Direct payment (PES)</td>\n",
       "      <td>Forest, Agriculture (Crop)</td>\n",
       "      <td>Incentive</td>\n",
       "      <td>1 (Sembrando Vida)</td>\n",
       "      <td>pesos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019 ACUERDO por el que se emiten los Lineamie...</td>\n",
       "      <td>El sujeto de derecho, recibirá en especie las ...</td>\n",
       "      <td>Supplies</td>\n",
       "      <td>Forest, Agriculture (Crop)</td>\n",
       "      <td>Incentive</td>\n",
       "      <td>1 (Sembrando Vida)</td>\n",
       "      <td>recibir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  \\\n",
       "0  2019 ACUERDO por el que se emiten los Lineamie...   \n",
       "1  2019 ACUERDO por el que se emiten los Lineamie...   \n",
       "2  2019 ACUERDO por el que se emiten los Lineamie...   \n",
       "3  2019 ACUERDO por el que se emiten los Lineamie...   \n",
       "4  2019 ACUERDO por el que se emiten los Lineamie...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Generar empleo y garantizara la población camp...   \n",
       "1  \\nEl Programa incentivará a los sujetos agrari...   \n",
       "2  Los sujetos agrarios beneficiados por el progr...   \n",
       "3  El sujeto de derecho, recibirá un apoyo económ...   \n",
       "4  El sujeto de derecho, recibirá en especie las ...   \n",
       "\n",
       "                                Incentive Instrument  \\\n",
       "0  Direct payment (PES), Credit, Technical assist...   \n",
       "1  Direct payment (PES), Credit, Technical assist...   \n",
       "2                     Supplies, Technical assistance   \n",
       "3                              Direct payment (PES)    \n",
       "4                                         Supplies     \n",
       "\n",
       "                Land Use Type   Category     Unique Policy #  \\\n",
       "0  Forest, Agriculture (Crop)  Incentive  1 (Sembrando Vida)   \n",
       "1  Forest, Agriculture (Crop)  Incentive  1 (Sembrando Vida)   \n",
       "2  Forest, Agriculture (Crop)  Incentive  1 (Sembrando Vida)   \n",
       "3  Forest, Agriculture (Crop)  Incentive  1 (Sembrando Vida)   \n",
       "4  Forest, Agriculture (Crop)  Incentive  1 (Sembrando Vida)   \n",
       "\n",
       "                                           Key words relevant sentences  \\\n",
       "0  insumo, crédito, capacitación, asistencia técnica                NaN   \n",
       "1                                         incentivar                NaN   \n",
       "2  apoyo económico, apoyos en especie, insumos, h...                NaN   \n",
       "3                                              pesos                NaN   \n",
       "4                                            recibir                NaN   \n",
       "\n",
       "  Key Words  \n",
       "0       NaN  \n",
       "1       NaN  \n",
       "2       NaN  \n",
       "3       NaN  \n",
       "4       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(r'WRI_Policy_Tags (1).xlsx', sheet_name = None)\n",
    "df = None\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(df,pd.DataFrame):\n",
    "            df = value\n",
    "        else:\n",
    "            df = df.append(value)\n",
    "else:\n",
    "    df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of number of words per sentence')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = df[\"relevant sentences\"].apply(lambda x: x.split(\";\") if isinstance(x,str) else x)\n",
    "sentence = []\n",
    "\n",
    "for elem in sentences:\n",
    "    if isinstance(elem,float) or len(elem) == 0:\n",
    "        continue\n",
    "    elif isinstance(elem,list):\n",
    "        for i in elem:\n",
    "            if len(i.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sentence.append(i.strip())\n",
    "    else:\n",
    "        if len(elem.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            sentence.append(elem.strip())\n",
    "\n",
    "sentence\n",
    "words_per_sentence = [len(x.split(\" \")) for x in sentence]\n",
    "plt.hist(words_per_sentence, bins = 50)\n",
    "plt.title(\"Histogram of number of words per sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n"
     ]
    }
   ],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "\n",
    "noise_words = []\n",
    "stopwords_corpus = nltk.corpus.stopwords\n",
    "sp_stop_words = stopwords_corpus.words('spanish')\n",
    "noise_words.extend(sp_stop_words)\n",
    "print(len(noise_words))\n",
    "\n",
    "if \"no\" in noise_words:\n",
    "    noise_words.remove(\"no\")\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(''.join(sentence))\n",
    "word_freq = Counter(tokenized_words)\n",
    "# word_freq.most_common(20)\n",
    "# list(ngrams(tokenized_words, 3))\n",
    "\n",
    "word_tokens_clean = [re.findall(r\"[a-zA-Z]+\",each) for each in tokenized_words if each.lower() not in noise_words and len(each.lower()) > 1]\n",
    "word_tokens_clean = [each[0].lower() for each in word_tokens_clean if len(each)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building queries with Parts-Of-Speech\n",
    "\n",
    "The following functions take a specific word and find the next or previous words according to the POS tags.\n",
    "\n",
    "An example is shown below with the text: <br>\n",
    "\n",
    "text = \"Generar empleo y garantizara la población campesina el bienestar y su participación e incorporación en el desarrollo nacional, y fomentará la actividad agropecuaria y forestal para el óptimo uso de la tierra, con obras de infraestructura, insumos, créditos, servicios de capacitación y asistencia técnica\" <br>\n",
    "\n",
    "next_words(text, \"empleo\", 3) <br>\n",
    "prev_words(text, \"garantizara\", 6) <br>\n",
    "\n",
    "Will return: <br>\n",
    "\n",
    ">['garantizara', 'población', 'campesina'] <br>\n",
    ">['Generar', 'empleo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = es_core_news_md.load()\n",
    "\n",
    "def ExtractInteresting(sentence, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "#     interesting = [k for k,v in nltk.pos_tag(words) if v in match]\n",
    "    doc = nlp(sentence)\n",
    "    interesting = [k.text for k in doc if k.pos_ in match]\n",
    "    return(interesting)\n",
    "\n",
    "def next_words(sentence, word, num_words, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "\n",
    "    items = list()\n",
    "    doc = nlp(sentence)\n",
    "    text = [i.text for i in doc]\n",
    "\n",
    "    if word not in text: return \"\"\n",
    "    \n",
    "    idx = text.index(word)\n",
    "    for num in range(num_words):\n",
    "        \n",
    "        pos_words = [k.text for k in doc[idx:] if k.pos_ in match]\n",
    "        if len(pos_words) > 1: \n",
    "            items.append(pos_words[1])\n",
    "            idx = text.index(pos_words[1])\n",
    "    \n",
    "    return items\n",
    "    \n",
    "def prev_words(sentence, word, num_words, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "    \n",
    "    items = list()\n",
    "    doc = nlp(sentence)\n",
    "    text = [i.text for i in doc]\n",
    "\n",
    "    if word not in text: return \"\"\n",
    "    \n",
    "    idx = text.index(word)\n",
    "    for num in range(num_words):\n",
    "        pos_words = [k.text for k in doc[:idx] if k.pos_ in match]\n",
    "        if len(pos_words) >= 1: \n",
    "            items.insert(0, pos_words[-1]) #Add element in order and take the last element since it is the one before the word\n",
    "            idx = text.index(pos_words[-1])\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams size\n",
    "We define the size of the n-gram that we want to find. The larger it is, the less frequent it will be, unless we substantially increase the number of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('manejo bosques naturales', 12), ('tierras vocaci forestal', 8), ('ministerio finanzas p', 8), ('bosques naturales fines', 8), ('programa incentivos forestales', 7), ('instituto nacional bosques', 7), ('plan manejo forestal', 7), ('poseedores peque extensiones', 6), ('establecimiento mantenimiento plantaciones', 6), ('mantenimiento plantaciones forestales', 6), ('otorgar incentivos forestales', 6), ('cuyas solicitudes ingreso', 6), ('solicitudes ingreso programa', 6), ('ingreso programa aprobadas', 6), ('programa aprobadas a', 6), ('naturales fines producci', 5), ('nacional bosques inab', 5), ('seg plan manejo', 5), ('aprobadas a proyectos', 5), ('incentivos forestales poseedores', 4)]\n"
     ]
    }
   ],
   "source": [
    "n_grams = 3\n",
    "\n",
    "top_k_ngrams(word_tokens_clean, n_grams, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "filename = \"Omdena_key.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the list of objects in the bucket that are relevant\n",
    "policy_list = s3.Bucket('wri-latin-policies').objects.all().filter(Prefix='full')\n",
    "\n",
    "## This allows to loop through the files\n",
    "\n",
    "# i = 0\n",
    "# for obj in s3.Bucket('wri-latin-policies').objects.all().filter(Prefix='full'):\n",
    "#     if i < 1: #Limit for testing purposes. if present the loop will go only through the first element\n",
    "#         key = \"Pre-processed/\" + obj.key.replace(\"full/\", '') + \".txt\" \n",
    "#         file = obj.get()['Body'].read() #get the file from S3\n",
    "#         pdf = PdfFileReader(BytesIO(file)) #load the file in pdf format if necessary\n",
    "#         for page in range(0, pdf.getNumPages()):\n",
    "#             pdf_page = pdf.getPage(page) #Retrieve the content of each page\n",
    "#             pdf_content = pdf_page.extractText() #Extract only the text of each page\n",
    "# #           HERE YOU SHOULD RUN YOUR PRE-PROCESSING PIPELINE AND ADD UP EVERY PAGE IN A VARIABLE called \"content\" as string\n",
    "#         s3.Object('wri-latin-policies', key).put(Body = content)#This will save all the contents in the string variable \"content\" into a txt file in the Pre-processed folder\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "The following functions are:\n",
    "- Get cosine similarity between two texts\n",
    "- Highlight: a function that receives the model, document, query and precision and returns all the highlights that are above that precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_name='xlm-r-100langs-bert-base-nli-mean-tokens'\n",
    "transformer_name = \"distiluse-base-multilingual-cased\"\n",
    "model = SentenceEmbeddings(transformer_name)\n",
    "\n",
    "def get_similarity(model, text1, text2):\n",
    "    '''\n",
    "    Given two texts, calculate the cosine similarity between their sentence embeddings.\n",
    "    '''\n",
    "    text1_embedding = model.encode(text1)\n",
    "    text2_embedding = model.encode(text2)\n",
    "    return 1 - distance.cosine(text1_embedding, text2_embedding)\n",
    "\n",
    "\n",
    "def highlight(model, document, query, precision):\n",
    "\n",
    "    '''document must be the json or txt document to be able to extract page'''\n",
    "    highlights = []\n",
    "    scores = []\n",
    "    pages = []\n",
    "    \n",
    "## Modify this part to change the processing of the json / dict policy ---------------------------\n",
    "\n",
    "    if isinstance(document, dict): \n",
    "        for page_num, text in document.items():\n",
    "            \n",
    "            ## This section is preprocessing ---------------------------------------\n",
    "            page_num = page_num.split(\"_\")[1]\n",
    "            for sentence in text.split(\"\\n\\n\"):\n",
    "                sentence = re.sub(\"\\n\", \" \", sentence)\n",
    "                sentence = re.sub(\" +\", \" \", sentence)\n",
    "                sentence = sentence.strip()\n",
    "                if len(sentence) < 60:\n",
    "                    continue\n",
    "             ## ---------------------------------------------------------------------------\n",
    "             ## Next, get the scores and stores the highlights\n",
    "                \n",
    "                score = get_similarity(model, sentence, query)\n",
    "                if score > precision:\n",
    "                    highlights.append(sentence)\n",
    "                    scores.append(score)\n",
    "                    pages.append(page_num)\n",
    "        sorted_idxs = np.argsort(scores)[::-1]\n",
    "        highlights = [highlights[idx] for idx in sorted_idxs]\n",
    "        scores = [scores[idx] for idx in sorted_idxs]\n",
    "        pages = [pages[idx] for idx in sorted_idxs]\n",
    "\n",
    "        return highlights, scores, pages\n",
    "# -----------------------------------------------------------------------------\n",
    "    else:\n",
    "        preprocessor = TextPreprocessor()\n",
    "        clean_text = preprocessor.clean_sentence(document)\n",
    "        paragraphs = preprocessor.split_into_paragraphs(document)\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph = re.sub(\"\\n\", \" \", sentence)\n",
    "            paragraph = re.sub(\" +\", \" \", sentence)\n",
    "            paragraph = paragraph.strip()\n",
    "            if len(paragraph) < 60:\n",
    "                continue\n",
    "            score = get_similarity(model, paragraph, query)\n",
    "            if score > precision:\n",
    "                highlights.append(paragraph)\n",
    "                scores.append(score)\n",
    "        sorted_idxs = np.argsort(scores)[::-1]\n",
    "        highlights = [highlights[idx] for idx in sorted_idxs]\n",
    "        scores = [scores[idx] for idx in sorted_idxs]\n",
    "\n",
    "        return highlights, scores, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure module\n",
    "highlighter_class = \"sbert\"\n",
    "highlighter_id = \"X\"\n",
    "highlighter_query = \"beneficio económico\"\n",
    "highlighter_precision = 0.05\n",
    "\n",
    "# Instantiate models from the Omdena files\n",
    "# highlighter_class = highlighter_classes[highlighter_class]\n",
    "# highlighter = highlighter_class.load(highlighter_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_folder = r\"_____\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for policy in policy_list:  #Uncomment and comment the next 2 lines if the files are fetched from S3 bucket.\n",
    "\n",
    "highlight_list = []\n",
    "scores_list = []\n",
    "pages_list = []\n",
    "\n",
    "for policy in os.listdir(policy_folder):\n",
    "    policy = os.path.join(policy_folder, policy)\n",
    "    \n",
    "    if \"json\" in policy:\n",
    "        with open(policy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            policy = json.load(f)\n",
    "        highlights, score, pages = highlight(\n",
    "            policy, highlighter_query, highlighter_precision\n",
    "        )\n",
    "            \n",
    "    elif \"txt\" in policy:\n",
    "        policy = open(policy_path, \"r\", encoding=\"utf-8\")\n",
    "        highlights, score, pages = highlight(\n",
    "            policy, highlighter_query, highlighter_precision\n",
    "        )\n",
    "    \n",
    "    highlight_list.append(highlights)\n",
    "    scores_list.append(highlights)\n",
    "    pages_list.append(highlights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
