{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq3BTDvOtYMh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from pdf2image import convert_from_path, convert_from_bytes\n",
    "import pytesseract\n",
    "import json\n",
    "import jamspell\n",
    "from spellchecker import SpellChecker\n",
    "import boto3\n",
    "from nltk.tokenize import word_tokenize\n",
    "from src import PROJECT_ROOT\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rk3oS_aKtz4W"
   },
   "outputs": [],
   "source": [
    "def get_image1(file_path):\n",
    "    \"\"\"Get image out of pdf file_path. Splits pdf file into PIL images of each of its pages.\n",
    "    \"\"\"\n",
    "    return convert_from_path(file_path, 500, thread_count=-1)\n",
    "\n",
    "def get_image2(bytes_object):\n",
    "    \"\"\"Get image out of pdf file_path. Splits pdf file into PIL imagecommand string or list of command arguments to run inside the container after is created. The commands execute from the workspaceFolders of each of its pages.\n",
    "    \"\"\"\n",
    "    return convert_from_bytes(bytes_object, 500, fmt=\"tiff\",thread_count=-1)\n",
    "\n",
    "def export_ocr(text, file, extract, out):\n",
    "    \"\"\" Export ocr output text using extract method to file at out\n",
    "    \"\"\"\n",
    "    filename = f'{os.path.splitext(os.path.basename(file))[0]}_{extract}.txt'\n",
    "    with open(os.path.join(out, filename), 'w') as the_file:\n",
    "        the_file.write(text)\n",
    "\n",
    "def wrap_pagenum(page_text, page_num):\n",
    "    \"\"\" Wrap page_text with page_num tag\n",
    "    \"\"\"\n",
    "    return f\"<p n={page_num}>\" + page_text + \"</p>\"\n",
    "\n",
    "def split_paragraphs(doc_text):\n",
    "    \"\"\" Split extracted document text into paragraphs\n",
    "    Replace \\x0c (page break character) by \\n. Match 1 or more occurrences of \\n if\n",
    "    preceeded by one occurrence of \\n OR match 1 or more occurrences of \\s \n",
    "    (whitespace) if preceeded by one occurrence of \\n or match one occurrence of \n",
    "    \\n if it isn't followed by \\n.\n",
    "\n",
    "    TODO: add a component to capture heards, footers, titles, so on... \n",
    "    (like Omdena did)\n",
    "    \"\"\"\n",
    "    return re.sub(\"(?<=\\n)\\n+|(?<=\\n)\\s+|\\n(?!\\n)\", \" \", doc_text.replace(\"\\x0c\", \"\\n\"))\n",
    "\n",
    "def set_folder_tag(folder_number):\n",
    "    return (\"sv\" + str(folder_number) + \"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgJS0Vg3RjpL"
   },
   "source": [
    "## Text extraction from pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2653090,
     "status": "ok",
     "timestamp": 1606480312384,
     "user": {
      "displayName": "David S.",
      "photoUrl": "",
      "userId": "02293755057010724464"
     },
     "user_tz": 0
    },
    "id": "iLaOA4ROtxK2",
    "outputId": "614a6ee9-fc21-450b-cd3f-aa68f7f51fdd"
   },
   "outputs": [],
   "source": [
    "## Extraction from google drive folders\n",
    "input_folder = os.path.join(PROJECT_ROOT, \"tasks\", \"extract_text\", \"input\")\n",
    "output_folder = os.path.join(PROJECT_ROOT, \"tasks\", \"extract_text\", \"output\")\n",
    "\n",
    "filepaths = [os.path.join(input_folder, file) for file in os.listdir(input_folder) if os.path.splitext(file)[1] == \".pdf\"]\n",
    "filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DlaOaYHuFUI"
   },
   "outputs": [],
   "source": [
    "# pytesseract extraction\n",
    "for file in filepaths:\n",
    "    pages = get_image1(file)\n",
    "    text = \"\"  # initialize document text\n",
    "    for pageNum, imgBlob in enumerate(pages):\n",
    "        page_text = pytesseract.image_to_string(imgBlob, lang=\"spa\")  # extract text\n",
    "        text += wrap_pagenum(page_text, pageNum)  # wrap page number\n",
    "    split_paragraphs(text)  # split document text into paragraphscommand string or list of command arguments to run inside the container after is created. The commands execute from the workspaceFolder\n",
    "    export_ocr(text, file, \"pytesseract\", output_folder)  # write extracted text to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spellchecking with pyspellchecker\n",
    "This model will classify each word as misspelled if it isn't contained in a spanish vocabulary.\n",
    "\n",
    "Then it uses a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word. It then compares all permutations (insertions, deletions, replacements, and transpositions) to known words in a word frequency list. Those words that are found more often in the frequency list are more likely the correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(language='es')\n",
    "\n",
    "# find those words that may be misspelled\n",
    "misspelled = spell.unknown(words)\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    corr = spell.correction(word)\n",
    "    # Get a list of `likely` options\n",
    "    cand = spell.candidates(word)\n",
    "    \n",
    "    print(f\"Mispelled word: {word} - Possible candidates: {cand} - Correction: {corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spellcheking using JamSpell\n",
    "\n",
    "- **Training the model is required as there's no pre-trained spanish jamspell model**. Instructions on how to do it at https://github.com/bakwc/JamSpell#train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use JamSpell\n",
    "corrector = jamspell.TSpellCorrector()\n",
    "\n",
    "# Check if model file exists\n",
    "modelFile = 'en.bin'\n",
    "if os.path.exists(modelFile):\n",
    "    corrector.LoadLangModel('en.bin')  # load the trained model\n",
    "\n",
    "    corrector.FixFragment('I am the begt spell cherken!')\n",
    "    # u'I am the best spell checker!'\n",
    "\n",
    "    corrector.GetCandidates(['i', 'am', 'the', 'begt', 'spell', 'cherken'], 3)\n",
    "    # (u'best', u'beat', u'belt', u'bet', u'bent', ... )\n",
    "\n",
    "    corrector.GetCandidates(['i', 'am', 'the', 'begt', 'spell', 'cherken'], 5)\n",
    "    # (u'checker', u'chicken', u'checked', u'wherein', u'coherent', ...)\n",
    "else:\n",
    "    print(\"modelFile contains inexistent path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR-6DeqtSHrt"
   },
   "source": [
    "## Extraction for S3 Bucket folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwABpNxeMdo5"
   },
   "outputs": [],
   "source": [
    "# Connection to AWS\n",
    "KEY = os.environ.get(\"S3_BUCKET\")\n",
    "SECRET = os.environ.get(\"SECRET_KEY\")\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 3323,
     "status": "error",
     "timestamp": 1606493325008,
     "user": {
      "displayName": "David S.",
      "photoUrl": "",
      "userId": "02293755057010724464"
     },
     "user_tz": 0
    },
    "id": "KFhuSLfdxQ0W",
    "outputId": "7778d024-864a-4e04-cf90-f8d02dac1382"
   },
   "outputs": [],
   "source": [
    "folder = 1 #This is the number of the folder in the S3 Bucket there are 10, from 1 to 10\n",
    "i = 0\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix='full'):\n",
    "    if set_folder_tag(folder) in obj.key and obj.key.replace(\"full/\" + set_folder_tag(folder), \"\") != \"\": #To run only over the desired folders\n",
    "        print(i, \"**\", obj.key)\n",
    "        key = \"text-extraction/\" + obj.key.replace(\"full/\"+  set_folder_tag(folder), \"\") + \".txt\"\n",
    "        file = obj.get()['Body'].read() #get the file from S3\n",
    "        pages = get_image2(file)\n",
    "        text = \"\"  # initialize document text\n",
    "        for pageNum, imgBlob in enumerate(pages):\n",
    "            page_text = pytesseract.image_to_string(imgBlob, lang=\"spa\")\n",
    "            text += wrap_pagenum(page_text, pageNum)  # wrap page number\n",
    "        content = split_paragraphs(text)  # extract text\n",
    "        s3.Object('wri-latin-talent', key).put(Body = content)#This will save all the contents in the string variable \"content\" into a txt file in the Pre-processed folder\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuDK-IL2Qw5t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OCR_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
